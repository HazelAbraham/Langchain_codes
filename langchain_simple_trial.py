# -*- coding: utf-8 -*-
"""langchain_simple_trial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U-6AuMSZ0eMD8VLHC3wqHLWvVlnlID7S
"""

import os
import warnings
warnings.filterwarnings('ignore')
import langchain

from google.colab import userdata
userdata.get('langchain_key')

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-groq

from langchain_core.messages import HumanMessage, SystemMessage
import langchain_groq

from langchain_groq import ChatGroq
groq_api_key = userdata.get('langchain_key')
model =ChatGroq(model="llama-3.1-8b-instant", groq_api_key=groq_api_key)

message=[SystemMessage(content="Hi you are nice Bot"),HumanMessage(content="Hi, how are you buddy?")]

message2=[SystemMessage(content="Hi you are nice Bot and you answer everything in brief"),HumanMessage(content="Hi, how are you buddy?")]

model.invoke(message)

#### Use of of output parser ..
# The Output parser  parsers the LLM  Result into the top likely string

from langchain_core.output_parsers import StrOutputParser

### Create a class  for this object
parser = StrOutputParser()



resp1=model.invoke(message)
parser.invoke(resp1)

###  what is chaining of   LCEL ---- Lanhgchain  expression language ...
### effective  way to provide declariative  and highly effective  way to chain runnables --- fiundamentals  , modular
chain= model| parser

chain.invoke(message)

##  what is prompting ....  passing simple  a message or argument    as an input ..
## Initalisy we  just send a message ..

## Chat prompt template  designed to create  chat based prompt as well as structured prompts
## For the LLMs .. It is usually  a role based messgae ,  it has dynamic , system message etc..


from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from google.colab import userdata

groq_api_key = userdata.get('langchain_key')
model =ChatGroq(model="llama-3.1-8b-instant", groq_api_key=groq_api_key)


template = ChatPromptTemplate.from_messages([
    # System message sets the persona and rules
    ("system", "You are an expert at translating English to French. Only provide the translation."),

    # Human message contains the dynamic user input
    ("human", "Translate this sentence: {text_to_translate}"),
])
prompt_value=template.invoke({"text_to_translate":"I am very lucky."})

parser = StrOutputParser() # Define parser here
parser.invoke(model.invoke(prompt_value))

## Chaining
# Create the LCEL Chain: Prompt -> Model -> Parser
translation_chain = template | model | parser
translation_chain.invoke({"text_to_translate":"I am very lucky."})

# Commented out IPython magic to ensure Python compatibility.

### Agents ..

# %pip install langgraph

from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain-community
# %pip install tavily-python

from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver


from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage

from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import HumanMessage

### I do not have acess to the Tavily Search api  so it will show error..

## Initalise the object to hadle the memory --- stores the conversational  history ..
memory = MemorySaver()


## uses the TavilySearch  api key which searche sthe max two results
search=TavilySearchResults( max_results=2)



tools=[search]


## Agent Executor which orcjesyrates the workflow
## checkpointer   important
agent_executor=create_react_agent(model,tools,checkpointer=memory)


config = {"configurable": {"thread_id": "abc123"}}



for chunk in agent_executor.stream({"messages":[HumanMessage(content="hi im Hazel! I am abudanat")]}, config):
    print(chunk)
    print("----")

for chunk in agent_executor.stream({"messages": [HumanMessage(content=" Tell me how can I manifest?")]}, config):
    print(chunk)
    print("----")